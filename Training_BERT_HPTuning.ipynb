{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import silence_tensorflow\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import skopt\n",
    "\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Google Bert Libraries\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes logs from previous runs\n",
    "!rm -rf ./logs/\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Output directory and GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n"
     ]
    }
   ],
   "source": [
    "# Set output directory for saving model data during training\n",
    "OUTPUT_DIR = \"model/OUTPUT_DIR_BERT\"\n",
    "\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Coloumn, Label Column\n",
    "DATA_COLUMN = 'tweet'\n",
    "LABEL_COLUMN = 'subtask_a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>she should ask a few native americans what the...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>go home your drunk!!! magna trump2020</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>amazon is investigation chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>someone should havetaken this piece of shit to...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>drama wanted liberals a illegal to move into r...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a\n",
       "0  86426  she should ask a few native americans what the...       OFF\n",
       "1  90194              go home your drunk!!! magna trump2020       OFF\n",
       "2  16820  amazon is investigation chinese employees who ...       NOT\n",
       "3  62688  someone should havetaken this piece of shit to...       OFF\n",
       "4  43605  drama wanted liberals a illegal to move into r...       NOT"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('data/X_processed.csv')\n",
    "y_tmp = pd.read_csv('data/y_processed.csv')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subtask_a\n",
       "0          1\n",
       "1          1\n",
       "2          0\n",
       "3          1\n",
       "4          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform binary string labels to integers [0, 1]\n",
    "y = pd.DataFrame(y_tmp['subtask_a'].apply(lambda x: 1 if x==\"OFF\" else 0))\n",
    "X['subtask_a'] = X['subtask_a'].apply(lambda x: 1 if x==\"OFF\" else 0)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and test set with sklearn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Input data to BERT-Embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "InputExamples_train = X_train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1).to_frame()\n",
    "InputExamples_test = X_test.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1).to_frame()\n",
    "InputExamples_val = X_val.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert list to values for computation purposes\n",
    "input_list_train = InputExamples_train.values.tolist()\n",
    "for i in range(len(input_list_train)):\n",
    "    input_list_train[i] = input_list_train[i][0]\n",
    "\n",
    "input_list_test = InputExamples_test.values.tolist()\n",
    "for i in range(len(input_list_test)):\n",
    "    input_list_test[i] = input_list_test[i][0]\n",
    "\n",
    "input_list_val = InputExamples_val.values.tolist()\n",
    "for i in range(len(input_list_val)):\n",
    "    input_list_val[i] = input_list_val[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained BERT Model form TF hub\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BERT Tokenizer and Bert vocab file\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long (mor ethan sufficient for Twitter posts)\n",
    "max_seq_length = 128\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally convert Input to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "feature_train = bert.run_classifier.convert_examples_to_features(input_list_train, label_list, max_seq_length, tokenizer)\n",
    "feature_test = bert.run_classifier.convert_examples_to_features(input_list_test, label_list, max_seq_length, tokenizer)\n",
    "feature_val = bert.run_classifier.convert_examples_to_features(input_list_val, label_list, max_seq_length, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define architecture of DL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Input and Output layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a classification model\n",
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "\n",
    "\n",
    "    bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "    bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "    print(output_layer.shape)  \n",
    "    print(output_layer.shape[-1].value)\n",
    "    print(type(output_layer.shape[-1]))\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # Specify training on GPU resource\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "\n",
    "            # Dropout helps prevent overfitting\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "            logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "            # Convert labels into one-hot encoding\n",
    "            one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "            predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "            # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "            if is_predicting:\n",
    "                return (predicted_labels, log_probs)\n",
    "\n",
    "            # If we're train/eval, compute loss between predicted and actual label\n",
    "            per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "            loss = tf.reduce_mean(per_example_loss)\n",
    "            return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function that builds and returns model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# HYPERPARAMTER\n",
    "dim_learning_rate = Real(low=1e-6, high=1e-1, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_batch_size = Categorical(categories=[16, 32, 64], \n",
    "                             name='batch_size')\n",
    "dim_num_epochs = Integer(low=3, high=15, name='num_epochs')\n",
    "dim_warmup_proportion = Real(low=0.05, high=0.6, name='warmup_proportion')\n",
    "dim_max_seq_length = Integer(low=20, high=140, name='max_seq_length')\n",
    "\n",
    "# Define list that spans Hyperparamter Space\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_batch_size,\n",
    "              dim_num_epochs,\n",
    "              dim_warmup_proportion,\n",
    "              dim_max_seq_length\n",
    "             ]\n",
    "# Specify initial parameters that optimizer will try first (x0)\n",
    "default_parameters = [2e-5, 32, 3, 0.1, 140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify GPU Settings and input to RunConfig\n",
    "config=tf.ConfigProto()\n",
    "config.log_device_placement=True\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "# Establish Run Configuration \n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,                            # directory to which models and checkpoints will be saved\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,          \n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,  \n",
    "    session_config=config)\n",
    "\n",
    "#tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function that gets called by Bayesian HP Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(dimensions):\n",
    "    print(f'\\nBeginning Training!\\n')\n",
    "    learning_rate = dimensions[0]\n",
    "    batch_size = dimensions[1]\n",
    "    num_epochs = dimensions[2]\n",
    "    warmup_proportion = dimensions[3]\n",
    "    max_seq_length = 128\n",
    "    print('Learning Rate:     {}'.format(learning_rate))\n",
    "    print('Batch Size:        {}'.format(batch_size))\n",
    "    print('Num Epochs:        {}'.format(num_epochs))\n",
    "    print('Warmup Proportion: {}'.format(warmup_proportion))\n",
    "\n",
    "    num_train_steps = int(len(feature_train) / batch_size * num_epochs)\n",
    "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "    \n",
    "\n",
    "    \n",
    "    # ACTUAL MODEL TRAINING STARTS HERE\n",
    "    with tf.Session() as sess: \n",
    "        model_fn = model_fn_builder(\n",
    "            num_labels=len(label_list),\n",
    "            learning_rate=learning_rate,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn=model_fn,\n",
    "            config=run_config,\n",
    "            params={\"batch_size\": batch_size})\n",
    "\n",
    "        train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "            features=feature_train,\n",
    "            seq_length=max_seq_length,\n",
    "            is_training=True,\n",
    "            drop_remainder=False)\n",
    "\n",
    "        current_time = datetime.now()\n",
    "        estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "        val_input_fn = run_classifier.input_fn_builder(\n",
    "        features=feature_val,\n",
    "        seq_length=max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "        results = estimator.evaluate(input_fn=val_input_fn, steps=None)\n",
    "    for acc in results:\n",
    "        print(acc, ':', results[acc])\n",
    "    print(\"\\nTraining took time \", datetime.now() - current_time)\n",
    "    return -results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning Training!\n",
      "\n",
      "Learning Rate:     2e-05\n",
      "Batch Size:        32\n",
      "Num Epochs:        3\n",
      "Warmup Proportion: 0.1\n",
      "(?, 768)\n",
      "768\n",
      "<class 'tensorflow.python.framework.tensor_shape.Dimension'>\n",
      "auc : 0.5016499\n",
      "eval_accuracy : 0.6603774\n",
      "f1_score : 0.49645385\n",
      "false_negatives : 338.0\n",
      "false_positives : 22.0\n",
      "loss : 0.6545831\n",
      "precision : 0.3529412\n",
      "recall : 0.034285713\n",
      "true_negatives : 688.0\n",
      "true_positives : 12.0\n",
      "global_step : 7943\n",
      "\n",
      "Training took time  0:08:48.701154\n",
      "\n",
      "Beginning Training!\n",
      "\n",
      "Learning Rate:     6.3467721326444465e-06\n",
      "Batch Size:        64\n",
      "Num Epochs:        4\n",
      "Warmup Proportion: 0.4693355342621294\n",
      "(?, 768)\n",
      "768\n",
      "<class 'tensorflow.python.framework.tensor_shape.Dimension'>\n",
      "auc : 0.499497\n",
      "eval_accuracy : 0.65943396\n",
      "f1_score : 0.49645385\n",
      "false_negatives : 340.0\n",
      "false_positives : 21.0\n",
      "loss : 0.6619486\n",
      "precision : 0.32258064\n",
      "recall : 0.028571429\n",
      "true_negatives : 689.0\n",
      "true_positives : 10.0\n",
      "global_step : 7943\n",
      "\n",
      "Training took time  0:05:27.197134\n",
      "\n",
      "Beginning Training!\n",
      "\n",
      "Learning Rate:     0.05445584020495123\n",
      "Batch Size:        16\n",
      "Num Epochs:        13\n",
      "Warmup Proportion: 0.44412386147471317\n",
      "(?, 768)\n",
      "768\n",
      "<class 'tensorflow.python.framework.tensor_shape.Dimension'>\n",
      "auc : 0.49883297\n",
      "eval_accuracy : 0.65660375\n",
      "f1_score : 0.49645385\n",
      "false_negatives : 338.0\n",
      "false_positives : 26.0\n",
      "loss : 0.6553567\n",
      "precision : 0.31578946\n",
      "recall : 0.034285713\n",
      "true_negatives : 684.0\n",
      "true_positives : 12.0\n",
      "global_step : 7943\n",
      "\n",
      "Training took time  0:03:03.057510\n",
      "\n",
      "Beginning Training!\n",
      "\n",
      "Learning Rate:     0.0791185959235732\n",
      "Batch Size:        16\n",
      "Num Epochs:        5\n",
      "Warmup Proportion: 0.37473301597912956\n",
      "(?, 768)\n",
      "768\n",
      "<class 'tensorflow.python.framework.tensor_shape.Dimension'>\n"
     ]
    }
   ],
   "source": [
    "# Function calls fitness function n_calls times and \n",
    "# samples each run from the defined Hyperparamter space (dimensions).\n",
    "gp_result = gp_minimize(func=fitness, \n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the set of best performing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = gp_result.x[0]\n",
    "batch_size = gp_result.x[1]\n",
    "num_epochs = gp_result.x[2]\n",
    "warmup_proportion = gp_result.x[3]\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate best performing model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Learning Rate:     {}'.format(learning_rate))\n",
    "print('Batch Size:        {}'.format(batch_size))\n",
    "print('Num Epochs:        {}'.format(num_epochs))\n",
    "print('Warmup Proportion: {}'.format(warmup_proportion))\n",
    "\n",
    "num_train_steps = int(len(feature_train) / batch_size * num_epochs)\n",
    "num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "with tf.Session() as sess:\n",
    "    model_fn = model_fn_builder(\n",
    "        num_labels=len(label_list),\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params={\"batch_size\": batch_size})\n",
    "\n",
    "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "        features=feature_train,\n",
    "        seq_length=max_seq_length,\n",
    "        is_training=True,\n",
    "        drop_remainder=False)\n",
    "    print(f'Beginning Training!')\n",
    "    current_time = datetime.now()\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "    print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=feature_test,\n",
    "    seq_length=max_seq_length,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "    labels = [0, 1]\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "    print(input_examples[i].label for i in range(len(in_sentences)))\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, max_seq_length, tokenizer)\n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=max_seq_length, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That asshole was absolutely awful\",\n",
    "  \"The Democrat Caucus is an unmitigated disaster. Nothing works, just like they ran the Country. Remember the  5 Billion Dollar Obamacare Website, that should have cost 2% of that. The only person that can claim a very big victory in Iowa last night is “Trump”.\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = getPrediction(pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
